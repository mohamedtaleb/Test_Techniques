# -*- coding: utf-8 -*-
"""Test_Technique_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ntqud00fHD2nPyy7BxfJnRrYe4LcdEDG

# Part I : Quel(le) data scientist êtes-vous ?
## Contexte de l’analyse

Elu métier le plus sexy par la Harvard Business Review en octobre 2012, le data scientist représente un profil rare qui exige de nombreuses compétences.

A partir d'un dataset Aquila, vous réaliserez :
- un clustering non supervisé afin d'identifier 2 groupes de profils techniques distinctes
- une prédiction des profils dont le métier n'est pas labellisé


## Données
data.csv contient 6 variables : 
    - 'Entreprise' correspond à une liste d'entreprises fictive
    - 'Metier' correspond au métier parmi data scientist, lead data scientist, data engineer et data architecte
    - 'Technologies' correspond aux compétences maîtrisées par le profil
    - 'Diplome' correspond à son niveau scolaire (Bac, Master, PhD,...)
    - 'Experience' correspond au nombre d'années d'expériences
    - 'Ville' correspond au lieu de travail
    


## Répondez aux questions 

Bonne chance!
"""

# Import des libraries classique (numpy, pandas, ...)
import pandas as pd
import numpy as np
import re
import sklearn as sk
import seaborn as sb
from matplotlib import pyplot as plt
plt.style.use('ggplot')


from sklearn import preprocessing
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import classification_report, confusion_matrix

"""### 1) Importer le tableau de données dans un dataframe """

# Importer les données dans un dataframe
df = pd.read_csv('/content/data/data.csv',sep=",",decimal=",")

# Afficher un échantillon aléatoire d'éléments 
df.sample(5)

"""### 2) Combien y a t-il d'observations dans ce dataset? Y a t-il des valeurs manquantes?

###   2.1 Récupérer le nombre d'observations dans ce dataset
"""

print("Le nombre total d'obsercations dans ce dataset : {}".format(df.shape[0]))

"""### 2.2 Verifier s'il y a des valeurs manquantes"""

df.isna().any()

"""### 2.3 Compter le nombre des valeurs manquantes par variable """

print("le nombre des valeurs manquantes par variable est :")
df.isna().sum()

"""### 3) Réaliser l'imputation des valeurs manquantes pour la variable "Experience" avec : 
- la valeur médiane pour les data scientists
- la valeur moyenne pour les data engineers

#### 3.1 Compter le nombre des valeurs manquantes pour la variable "Experience"
"""

print("Le nombre de valeurs manquantes avant l'imputation pour les data scientists :{}".format(df.loc[df.Metier=='Data scientist','Experience'].isna().sum()))
print("Le nombre de valeurs manquantes avant l'imputation pour les ddata engineers :{}".format(df.loc[df.Metier=='Data engineer','Experience'].isna().sum()))

"""#### 3.2 Réaliser l'imputation des valeurs manquantes"""

df.loc[df.Metier=='Data scientist','Experience']=df.loc[df.Metier=='Data scientist','Experience'].fillna(df[df['Metier'] == 'Data scientist']['Experience'].dropna().median())
df.loc[df.Metier=='Data engineer','Experience']=df.loc[df.Metier=='Data engineer','Experience'].fillna(df[df['Metier'] == 'Data engineer']['Experience'].dropna().mean())

"""#### 3.3 Vérification"""

print("Le nombre de valeurs manquantes après l'imputation pour les data scientists :{}".format(df.loc[df.Metier=='Data scientist','Experience'].isna().sum()))
print("Le nombre de valeurs manquantes après l'imputation pour les ddata engineers :{}".format(df.loc[df.Metier=='Data engineer','Experience'].isna().sum()))

"""### 4) Combien d'années d'expériences ont, en moyenne, chacun des profils : le data scientist, le lead data scientist et le data engineer en moyenne?"""

moy_ds=df.loc[df.Metier=='Data scientist','Experience'].dropna().mean()
moy_lds=df.loc[df.Metier=='Lead data scientist','Experience'].dropna().mean()
moy_de=df.loc[df.Metier=='Data engineer','Experience'].dropna().mean()
print("Les années moyennes pour chaque profil : \n")
print("Lead data scientist :",round(moy_lds,3))
print("Data enginner       :",round(moy_de,3))
print("Data scientist      :",round(moy_ds,3))

"""### 5) Faire la représentation graphique de votre choix afin de comparer le nombre moyen d'années d'expériences pour chaque métier"""

plt.figure(figsize=(6, 6))
plt.style.use('classic')
sb.barplot(x=df['Metier'].value_counts().keys().drop('Data architecte'),y=[moy_ds,moy_lds,moy_de])
plt.ylabel("La valeur moyenne\n")
plt.title("Le nombre moyen d'années d'expériences par métier\n")
plt.show()

"""### 6) Transformer la variable continue 'Experience' en une nouvelle variable catégorielle 'Exp_label' à 4 modalités: débutant, confirmé, avancé et expert
- Veuillez expliquer votre choix du règle de transformation.

#### Les valeurs manquantes restantes de la variable Experience doivent être remplies en premier lieu.
"""

moy_da=df.loc[df.Metier=='Data architecte','Experience'].dropna().mean()
df.loc[df.Metier=='Lead data scientist','Experience']=df.loc[df.Metier=='Lead data scientist','Experience'].fillna(moy_lds)
df.loc[df.Metier=='Data architecte','Experience']=df.loc[df.Metier=='Data architecte','Experience'].fillna(moy_da)

"""#### Transformation de la variable continue "Expérience" en une nouvelle variable catégorielle "Exp_label". 


"""

labels=['Debutant','Confirme','Avance','Expert']
df['Exp_label']=pd.cut(df['Experience'],bins=[-1,2,6,10,30], labels=labels)
df.tail(10)

plt.figure(figsize=(6, 6))
sb.barplot(x=df['Exp_label'].value_counts().keys(),y=df['Exp_label'].value_counts().values)
plt.ylabel("Le nombre d'employeurs \n")
plt.title("Le nombre d'employeurs par niveau.\n")
plt.show()

"""### 7) Quelles sont les 5 technologies les plus utilisées? Faites un graphique"""

most_used_tech=df['Technologies'].value_counts()[:5]
print("les 5 technologies les plus utilisées sont :\n")
print(most_used_tech)

plt.figure(figsize=(8, 4))
sb.barplot(x=most_used_tech.values,y=most_used_tech.keys())
plt.title("les 5 technologies les plus utilisées")
plt.show()

"""### 8) Réaliser une méthode de clustering non supervisée de votre choix pour faire apparaître 2 clusters que vous jugerez pertinents. Donnez les caractéristiques de chacun des clusters.
-  Justifier la performance de votre algorithme grace à une métrique.
-  Interpréter votre resultat.

#**Analyse du forme**

### On crée une copie du Dataset
"""

data=df.copy()

data.shape

"""### Etudier tous les éléments de chaque caractéristique


"""

for column in data:
  unique_val=data[column].unique()
  nr_unique_val=len(unique_val)
  if nr_unique_val<10:
    print("the number of value for feature {} : {} ---- {}".format(column,nr_unique_val,unique_val))
  else:
    print("the number of value for feature {} : {}".format(column,nr_unique_val))

"""### On prepare un pipeline qui fait l'encodage et la normalisation des données"""

# 1) Pipeline pour les variables numériques
numerical_pipeline=make_pipeline(SimpleImputer(),StandardScaler())

# 2) Pipeline pour les autres variables qui sont qualitatives
categorical_pipeline=make_pipeline(SimpleImputer(strategy='most_frequent'),OneHotEncoder())

numerical_features=['Experience']
categorical_features=['Technologies','Diplome','Exp_label']

# Construire un ColumnTransformer à partir des transformateurs donnés
preprocessor= make_column_transformer((numerical_pipeline,numerical_features),(categorical_pipeline,categorical_features))

data=data[['Experience','Technologies','Diplome','Exp_label']]
x_encoded=preprocessor.fit_transform(data)

"""### Initialisation de l'algorithme Kmeans et de notre modèle de clustering"""

kmeans =KMeans(n_clusters=2)
model= make_pipeline(preprocessor,kmeans)

# Entrainer le model
kmeans.fit(x_encoded)

"""## Les caracteristiques les plus imortantes des deux clusteres

### Il faut coder le Dataset avec une autre facon  pour pouvoir extraire les caracteristiques les plus imortantes qui ont l'impact de distinguer les clusters
"""

data1=data.copy()
data1=data1[['Experience','Technologies','Diplome','Exp_label']]
le = preprocessing.LabelEncoder()
df_le = data1.loc[:,['Experience']]
data1=data1.drop(['Experience'],axis=1)
for culumn in data1:
  df_le[culumn]=le.fit_transform(data1[culumn])
df_le.head()

"""### Rajouter la variable "Labels" au data-frame initial et Configuration des points dans le plan"""

labels = kmeans.labels_
B=df_le.copy()
B['labels']=labels

#des variables prises par paires
#On colore les points selon les labels
sb.pairplot(B,hue='labels')

"""### A partir ce graphique, et sur la diagonale principal, les fonctions de densité conditionnelle  permet de savoir quand les variables sont prises de maniere individuelle lesquels ont le plus d'impact sur la séparabilité des clusters <br>On peut constater donc que les caractistiques les plus imortantes à distinguer les deux clusteres sont : **Experience et Exp_label**

### Il est également possible de calculer les moyennes des caractéristiques par Label pour savoir comment elles diffèrent les deux clusters.<br> Cette technique peut être utile lorsque nous avons plusieurs variables
"""

#on crée tout d'abord deux sous-DataFrames pour cluster_1 et cluster_2
gb=B.groupby(labels)
#effectifs par classe 
print(gb.size())
#Moyennes par classe
gb.mean()

"""### Le rapport de correlation peut aussi aider à identifier les correlation entre les Labels et les caracteristiques (forte ou faible)<br> Le tableau montre qu'il y a une correlation forte entre la variable **Experience** et **Labels**, aussi il une correlation forte entre la variable **Exp_label** et **Labels**"""

B.corr()

"""## **Evaluation**

### La métrique utilisée est inertia
"""

print(kmeans.inertia_)

"""# Ce code à pour objectif d'identifier le nombre ideale des clusters """

inertie=[]
k_range= range(1,20)
for clustr in k_range:
  x_encoded=preprocessor.fit_transform(data)
  kmeans = KMeans(n_clusters=clustr, max_iter = 100, algorithm = 'auto')
  kmeans.fit(x_encoded)
  inertie.append(kmeans.inertia_)

plt.figure(figsize=(8,7))
plt.plot(k_range,inertie)
plt.xlabel("Nombre de de cluster")
plt.ylabel("la valeur d'inertia")
plt.show()

"""### 9) Réaliser la prédiction des métiers manquants dans la base de données par l'algorithme de votre choix
-  Justifier la performance de votre algorithme grace à une métrique.
-  Interpréter votre resultat.

## On affiche d'abord les métiers manquants dans la base de données
"""

print("Le nombre des métiers manquants dans la base de données est : ",len(df[df['Metier'].isna()]))
df[df['Metier'].isna()]

"""### On prépare les données d'entrainement et les données des métiers manquants"""

mum_category=preprocessing.LabelEncoder()
metier_manq= df.loc[:,'Metier'].isna()
x=np.array(df_le[~metier_manq])
y=mum_category.fit_transform(df.loc[~metier_manq,'Metier'].dropna())
#Metiers à predire
metier_a_predire=np.array(df_le[metier_manq])

x.shape, y.shape, metier_a_predire.shape

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.25, shuffle=True , random_state=0)

"""### Nous allons tester différents modèles d'apprentissage automatique afin de choisir le modèle ayant la plus grande précision pour la prédiction des métiers manquants.<br>Et pour chaque alogorithme, nous allons utiliser une technique pour sélectionner les meilleurs paramètres.

### 1. Model KNeighborsClassifier
"""

param_grid={'n_neighbors':np.arange(1,20),
            'metric':['euclidean','manhattan']}
grid=GridSearchCV(KNeighborsClassifier(),param_grid)
grid.fit(X_train,y_train)
model_KNN=grid.best_estimator_
print(grid.best_score_,grid.best_params_)

"""###2. Model DecisionTreeClassifier

"""

param_grid={'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}
grid=GridSearchCV(DecisionTreeClassifier(),param_grid)
grid.fit(X_train,y_train)
model_DT=grid.best_estimator_
print(grid.best_score_,grid.best_params_)

"""##3. Model RandomForestClassifier 


"""

param_grid = {
    'n_estimators'      : [320,330,340],
    'max_depth'         : [8, 9, 10, 11, 12],
    'random_state'      : [0],
    #'max_features': ['auto'],
    #'criterion' :['gini']
}

model_rfc = GridSearchCV(RandomForestClassifier(), param_grid, cv=10, n_jobs=-1)
model_rfc.fit(X_train,y_train)
print(model_rfc.best_score_,model_rfc.best_params_)
model_rfc=model_rfc.best_estimator_

"""Pour essayer d'améliorer les performances de notre modèle, nous pouvons tester une méthode qui consiste à former plusieurs modèles d'apprentissage automatique et à considérer ensuite toutes leurs prédictions """

model=VotingClassifier([('rf', model_rfc),
                          ('Tree',model_DT),
                          ('KNN',model_KNN)],
                         voting='soft')

for model in(model_rfc,model_DT,model_KNN,model):
  model.fit(X_train,y_train)
  print(model.__class__.__name__,model.score(X_test,y_test))

"""Cette technique de vote ne fonctionne pas bien dans notre cas, peut-être parce qu'il n'y a pas beaucoup de diversité entre les 3 modèles, c'est-à-dire qu'en termes de prédiction, ils nous diront tous la même chose, donc ils ne sont pas très diversifiés.
Dans ce cas, j'ai choisi le modèle qui a donné la meilleure performance (**modèle_DT**).
"""

y_pred = model_DT.predict(X_test)
c__matrix = confusion_matrix(y_test, y_pred) 
print('Matrice de confusion :\n')
print(c__matrix)
print('\n\n Rapport de classificiation :\n')
print(classification_report(y_test,y_pred))

"""##  Prédiction des métiers manquants dans la base de données"""

#Prédire les métiers manquants
Predicted_Metiers=model_DT.predict(metier_a_predire)
#Remplissage de la base de données par les métiers prédis
df.loc[~metier_manq, 'Predicted_Metiers'] = df.loc[:,'Metier']
df.loc[metier_manq, 'Predicted_Metiers'] = mum_category.inverse_transform(Predicted_Metiers)
#Affichage
print("Les résultats de la prediction des métiers manquants \n")
df[metier_manq]

